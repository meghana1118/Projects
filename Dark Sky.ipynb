{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DarkSky: Scraping weather data \n",
    "\n",
    "The aim of this project is to scrape weather data from multiple URLs and save it in as CSV,as tables MySQL and as collection in MongoDB.\n",
    "\n",
    "The Dark Sky API (https://darksky.net/dev) allows you to look up the weather anywhere on the globe.You can make 2 kinds of API calls; Forecast request and Time Machine Request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beloved Pandas\n",
    "import pandas as pd\n",
    "\n",
    "## Libraries needed to connect to URL and request for data \n",
    "import requests\n",
    "from pandas.io.json import json_normalize #Normalize semi-structured JSON data into a flat table.\n",
    " \n",
    "## Libraries needed to work with time\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Libraries needed to connect to MySQL\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "# Libraries needed to connect to MongoDB\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up intial files\n",
    "\n",
    "Read the file with store ID(105 IDs with unique latitude and longitude). Isolate all 3 columns to use them individually later. \n",
    "\n",
    "Base URL contails the API key and can be used to create list of URLs to get Future predicted data and Historical weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read excel file\n",
    "location=pd.read_csv(r'C:\\Users\\megha\\Documents\\Projects\\DarkSky\\store.csv') #105 store  IDs\n",
    "\n",
    "#API Key\n",
    "#authorization_code='ffb7c6d0d32876c01496218fbd979218'\n",
    "\n",
    "#isolate latitude and longitude\n",
    "lat,long=location['LATITUDE'],location['LONGITUDE'] #needed to create URls\n",
    "store_id=location['LOC_ID'] \n",
    "\n",
    "#base url\n",
    "base_url=\"https://api.darksky.net/forecast/ffb7c6d0d32876c01496218fbd979218/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Time Machine request \n",
    "\n",
    "Enter start date and end date for the which you need weather data and creat a list of dates, convert it to unix time as API calls are made using URLs with location and unixtime. Use the date in unix time, Location and base URL to create a list of URL you need to request to get the data. The data is in Json format and has to be normalized to a flat table to be converted to Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megha\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "#Past date weather\n",
    "#Get time for Time_machine date\n",
    "dates=[]\n",
    "\n",
    "start_date = date(2019, 12, 1)\n",
    "end_date = date(2019, 12, 5)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "while start_date <= end_date:\n",
    "    c=start_date.strftime(\"%d/%m/%Y\")\n",
    "    dates.append(c)\n",
    "    start_date += delta\n",
    "date_unix=[]\n",
    "for i in dates:\n",
    "    f=time.mktime(datetime.datetime.strptime(i, \"%d/%m/%Y\").timetuple())\n",
    "    date_unix.append(f)\n",
    "    \n",
    "#make list of URLs    \n",
    "url=''\n",
    "for i,j in zip(lat,long):\n",
    "    for k in date_unix:\n",
    "        url +=base_url+str(i)+str(',')+str(j)+str(',')+str(int(k))+str(\" \")\n",
    "    url_list=url.split(\" \")\n",
    "    url_list.remove('')\n",
    "    \n",
    "#Dataframe with old weather data\n",
    "Time_machine=pd.DataFrame()\n",
    "for url in url_list:\n",
    "    response=requests.get(url)\n",
    "    r=response.json()\n",
    "    df=json_normalize(r['currently'])\n",
    "    df['LATITUDE']=r['latitude']\n",
    "    df['LONGITUDE']=r['longitude']\n",
    "    Time_machine= Time_machine.append(df, ignore_index=True)\n",
    "    Time_machine['date'] = pd.to_datetime(Time_machine['time'],unit='s')\n",
    "\n",
    "Time_machine_data=pd.merge(location, Time_machine, on=['LATITUDE', 'LONGITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Request\n",
    "\n",
    "Create a list of URL using only location and base URL, forecast does not need time. Use the URL list to create multiple requests. The data is in Json format and has to be normalized to a flat table to be converted to Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecastorecast forecastor forecastututre\n",
    "#making a list oforecast URLs\n",
    "url_forecast=''\n",
    "for i,j in zip(lat,long):\n",
    "    url_forecast +=base_url+str(i)+str(',')+str(j)+str(\" \")\n",
    "    url_list_forecast=url_forecast.split(\" \")\n",
    "    url_list_forecast.remove('')\n",
    "\n",
    "#Dataforecastrame with new prediction data\n",
    "forecast=pd.Dataforecastrame()\n",
    "for url,i in zip(url_list_forecast,store_id):\n",
    "    response=requests.get(url)\n",
    "    r=response.json()\n",
    "    data=json_normalize(r,['daily','data'],['latitude','longitude'])\n",
    "    data['date'] = pd.to_datetime(data['time'],unit='s')\n",
    "    data['store_id']=i\n",
    "    forecast_data = forecast.append(data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_machine_data.to_csv('History.csv')\n",
    "forecast_data.to_csv('Forecasting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.sqlalchemy.org/en/13/core/engines.html\n",
    "#https://pythontic.com/pandas/serialization/mysql\n",
    "#moving both dataframes to MySQL database\n",
    "table1='DarkSkyForecast'\n",
    "table2='DarkSkyHistory'\n",
    "\n",
    "#create this schema in SQL prior to running program\n",
    "tableName='darksky'\n",
    "\n",
    "sqlEngine = create_engine('mysql+pymysql://root:megh@n@1811@localhost/darksky', pool_recycle=3600)\n",
    "\n",
    "dbConnection = sqlEngine.connect() \n",
    "try:\n",
    "    frameh = Time_machine_data.to_sql(table2, dbConnection, if_exists='fail');\n",
    "    framef = forecast_data.to_sql(table1, dbConnection, if_exists='fail')\n",
    "except ValueError as vx:\n",
    "    print(vx)\n",
    "except Exception as ex:   \n",
    "    print(ex)\n",
    "else:\n",
    "    print(\"Table %s created successfully.\"%tableName);   \n",
    "finally:\n",
    "    dbConnection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/20167194/insert-a-pandas-dataframe-into-mongodb-using-pymongo\n",
    "client = MongoClient('localhost', 27017)\n",
    "#or use\n",
    "#client = MongoClient('mongodb://localhost:27017')\n",
    "db=client['Weather_data']\n",
    "collection_1=db['Forecast']\n",
    "collection_2=db['History']\n",
    "\n",
    "collection_1.insert_many(Time_machine_data.to_dict('record'))\n",
    "collection_2.insert_many(forecast_data.to_dict('record'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
